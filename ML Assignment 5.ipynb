{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# THEROTICAL"
      ],
      "metadata": {
        "id": "Ljyh4YJHdU4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.1  What is Logistic Regression, and how does it differ from Linear Regression?'''"
      ],
      "metadata": {
        "id": "kf1IzcsJdaj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression is a supervised learning algorithm used for classification tasks, especially when the target variable is binary (e.g., 0 or 1, yes or no, spam or not spam).\n",
        "\n",
        "How Logistic Regression Works:\n",
        "\n",
        "* Instead of predicting a continuous value like linear regression, it predicts the probability that a given input belongs to a particular class.\n",
        "\n",
        "* It uses the sigmoid (logistic) function to convert the linear output into a probability between 0 and 1:\n",
        "\n",
        "  $$\n",
        "  P(y = 1 | X) = \\frac{1}{1 + e^{-(b_0 + b_1X_1 + b_2X_2 + \\dots + b_nX_n)}}\n",
        "  $$\n",
        "\n",
        "* If the predicted probability is greater than 0.5, the output is class 1; otherwise, class 0.\n",
        "\n",
        "Key Differences Between Logistic and Linear Regression:\n",
        "\n",
        "| Aspect            | Logistic Regression                        | Linear Regression                      |\n",
        "| -- |  | -- |\n",
        "| Output            | Probability (between 0 and 1)              | Continuous value                       |\n",
        "| Use case          | Classification (binary or multi-class)     | Regression (predicting numeric values) |\n",
        "| Algorithm Output  | Uses sigmoid to classify into categories   | Direct numeric prediction              |\n",
        "| Cost Function     | Log Loss (Cross-Entropy)                   | Mean Squared Error                     |\n",
        "| Decision boundary | Based on probability threshold (e.g., 0.5) | Not applicable                         |\n",
        "| Example           | Email spam detection                       | Predicting house prices                |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U1nFRVAZdmEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.2 What is the mathematical equation of Logistic Regression?'''"
      ],
      "metadata": {
        "id": "zmPA1Kn1eCvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mathematical equation of Logistic Regression is based on the sigmoid (logistic) function, which maps any real-valued number into a probability between 0 and 1.\n",
        "\n",
        "1. Linear Combination (like in Linear Regression):\n",
        "\n",
        "$$\n",
        "z = b_0 + b_1x_1 + b_2x_2 + \\dots + b_nx_n = \\mathbf{w}^T\\mathbf{x}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $z$ is the linear combination of features\n",
        "* $b_0$ is the intercept (bias)\n",
        "* $b_1, b_2, ..., b_n$ are the coefficients (weights)\n",
        "* $x_1, x_2, ..., x_n$ are the input features\n",
        "\n",
        "2. Sigmoid Function (to get probability):\n",
        "\n",
        "$$\n",
        "P(y = 1 \\mid x) = \\sigma(z) = \\frac{1}{1 + e^{-z}} = \\frac{1}{1 + e^{-(b_0 + b_1x_1 + \\dots + b_nx_n)}}\n",
        "$$\n",
        "\n",
        "This gives the probability that the output belongs to class 1.\n",
        "\n",
        "3. Decision Rule:\n",
        "\n",
        "* If $P(y = 1 \\mid x) \\geq 0.5$, predict class 1\n",
        "* If $P(y = 1 \\mid x) < 0.5$, predict class 0\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vEXgUouxeITk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.3 Why do we use the Sigmoid function in Logistic Regression?'''"
      ],
      "metadata": {
        "id": "OXP5Yj9CepYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the sigmoid function in logistic regression because it transforms the output of a linear equation into a probability value between 0 and 1, which is essential for binary classification tasks.\n",
        "\n",
        "Reasons for Using the Sigmoid Function:\n",
        "\n",
        "1. Probability Output\n",
        "   The sigmoid function converts the raw linear combination (which can be any real number) into a probability:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}} \\in (0, 1)\n",
        "   $$\n",
        "\n",
        "   This makes it suitable for estimating the likelihood that a sample belongs to class 1.\n",
        "\n",
        "2. Clear Decision Boundary\n",
        "   A threshold (commonly 0.5) can be applied to the output:\n",
        "\n",
        "   * If output ≥ 0.5 → predict class 1\n",
        "   * If output < 0.5 → predict class 0\n",
        "\n",
        "3. Smooth and Differentiable\n",
        "   The sigmoid function is smooth and differentiable, which allows gradient-based optimization (like gradient descent) during training.\n",
        "\n",
        "4. Monotonic Behavior\n",
        "   The function always increases, meaning a higher value of input $z$ leads to a higher probability, preserving the order.\n",
        "\n",
        "5. Center Around 0.5\n",
        "   When $z = 0$, the output is 0.5 — a neutral probability — making it intuitive for classification.\n",
        "\n",
        "\n",
        "\n",
        "In summary, the sigmoid function transforms linear predictions into interpretable probabilities, making it the core of logistic regression for binary outcomes.\n"
      ],
      "metadata": {
        "id": "iVjkMCQke4_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.4 What is the cost function of Logistic Regression?'''"
      ],
      "metadata": {
        "id": "Gb3jnEpofFNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cost function of Logistic Regression is the Log Loss, also known as Binary Cross-Entropy Loss.\n",
        "\n",
        "It measures how well the predicted probabilities match the actual class labels (0 or 1), penalizing incorrect predictions more as their confidence increases.\n",
        "\n",
        "\n",
        "\n",
        " Cost Function (Log Loss)\n",
        "\n",
        "For binary classification, the cost function is defined as:\n",
        "\n",
        "$$\n",
        "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $m$ = number of training examples\n",
        "* $y^{(i)}$ = actual label (0 or 1)\n",
        "* $\\hat{y}^{(i)}$ = predicted probability $\\sigma(z^{(i)})$\n",
        "* $\\theta$ = model parameters (weights)\n",
        "\n",
        "\n",
        "\n",
        " Why Not Use Mean Squared Error (MSE)?\n",
        "\n",
        "* MSE is used in linear regression, but not suitable for logistic regression because:\n",
        "\n",
        "  * It leads to non-convex cost functions, making optimization harder.\n",
        "  * It doesn't model probability effectively.\n",
        "\n",
        "\n",
        "\n",
        " Goal\n",
        "\n",
        "Minimize $J(\\theta)$, which is typically done using gradient descent to update the weights iteratively.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o79IZC77fQ9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.5 What is Regularization in Logistic Regression? Why is it needed?'''"
      ],
      "metadata": {
        "id": "W6D6eNJkfxbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization in logistic regression is a technique used to prevent overfitting by adding a penalty term to the cost function. This penalty discourages the model from learning overly complex patterns that may not generalize well to unseen data.\n",
        "\n",
        "\n",
        "\n",
        " Why is Regularization Needed?\n",
        "\n",
        "1. Prevents Overfitting\n",
        "   When the model becomes too complex (e.g., too many features or high coefficients), it may fit the noise in the training data. Regularization penalizes large weights, leading to a simpler, more generalizable model.\n",
        "\n",
        "2. Improves Generalization\n",
        "   Helps the model perform better on new, unseen data by controlling the complexity of the hypothesis.\n",
        "\n",
        "3. Feature Shrinkage\n",
        "   Some forms of regularization shrink irrelevant feature weights close to zero, effectively performing feature selection.\n",
        "\n",
        "\n",
        "\n",
        " Types of Regularization in Logistic Regression\n",
        "\n",
        "1. L2 Regularization (Ridge)\n",
        "\n",
        "Adds the squared magnitude of coefficients to the cost function:\n",
        "\n",
        "$$\n",
        "J(\\theta) = -\\frac{1}{m} \\sum \\left[ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right] + \\lambda \\sum \\theta_j^2\n",
        "$$\n",
        "\n",
        "* Keeps all features but shrinks coefficients.\n",
        "* Most commonly used in logistic regression.\n",
        "\n",
        "2. L1 Regularization (Lasso)\n",
        "\n",
        "Adds the absolute value of coefficients:\n",
        "\n",
        "$$\n",
        "J(\\theta) = -\\frac{1}{m} \\sum \\left[ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right] + \\lambda \\sum |\\theta_j|\n",
        "$$\n",
        "\n",
        "* Can set some coefficients exactly to zero → useful for feature selection.\n",
        "\n",
        "\n",
        "\n",
        " Summary\n",
        "\n",
        "Regularization is essential to make logistic regression models more robust, interpretable, and better-performing by controlling overfitting and encouraging simpler models.\n"
      ],
      "metadata": {
        "id": "YJTi2QF7f9Lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.6 Explain the difference between Lasso, Ridge, and Elastic Net regression'''"
      ],
      "metadata": {
        "id": "KRV24GkcgYqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a clear comparison of Lasso, Ridge, and Elastic Net regression—three regularization techniques used in linear and logistic regression to prevent overfitting and improve model generalization.\n",
        "\n",
        "\n",
        "\n",
        " 1. Ridge Regression (L2 Regularization)\n",
        "\n",
        "* Penalty term: Adds the sum of squared coefficients to the loss function.\n",
        "\n",
        "  $$\n",
        "  J(\\theta) = \\text{Loss} + \\lambda \\sum_{j=1}^{n} \\theta_j^2\n",
        "  $$\n",
        "\n",
        "* Effect:\n",
        "\n",
        "  * Shrinks all coefficients closer to zero.\n",
        "  * Does not eliminate any coefficients (i.e., keeps all features).\n",
        "  * Useful when all features are relevant but need regularization.\n",
        "\n",
        "\n",
        "\n",
        " 2. Lasso Regression (L1 Regularization)\n",
        "\n",
        "* Penalty term: Adds the sum of absolute values of coefficients.\n",
        "\n",
        "  $$\n",
        "  J(\\theta) = \\text{Loss} + \\lambda \\sum_{j=1}^{n} |\\theta_j|\n",
        "  $$\n",
        "\n",
        "* Effect:\n",
        "\n",
        "  * Shrinks some coefficients to exactly zero.\n",
        "  * Performs feature selection.\n",
        "  * Useful when only a few features are important.\n",
        "\n",
        "\n",
        "\n",
        " 3. Elastic Net Regression\n",
        "\n",
        "* Penalty term: Combination of L1 and L2 regularization.\n",
        "\n",
        "  $$\n",
        "  J(\\theta) = \\text{Loss} + \\lambda_1 \\sum |\\theta_j| + \\lambda_2 \\sum \\theta_j^2\n",
        "  $$\n",
        "\n",
        "* Effect:\n",
        "\n",
        "  * Balances between Ridge and Lasso.\n",
        "  * Handles multicollinearity better than Lasso.\n",
        "  * Selects variables and stabilizes model.\n",
        "\n",
        "\n",
        "\n",
        " Key Differences\n",
        "\n",
        "| Aspect            | Ridge              | Lasso              | Elastic Net               |\n",
        "| -- |  |  | - |\n",
        "| Penalty Type      | L2 (squared)       | L1 (absolute)      | L1 + L2                   |\n",
        "| Feature Selection | No                 | Yes                | Yes                       |\n",
        "| Coefficients      | Shrunk             | Some zero          | Some zero + shrink others |\n",
        "| Best When         | Many small effects | Few strong effects | Mixture of both           |\n",
        "\n",
        "\n",
        "\n",
        " Summary\n",
        "\n",
        "* Use Ridge when all features matter but need to avoid overfitting.\n",
        "* Use Lasso when you want a simpler model that automatically removes irrelevant features.\n",
        "* Use Elastic Net when you need both regularization and feature selection, especially with correlated predictors.\n",
        "\n"
      ],
      "metadata": {
        "id": "UjsO6gUDgyeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.7  When should we use Elastic Net instead of Lasso or Ridge?'''"
      ],
      "metadata": {
        "id": "uj0BWckOhAe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should use Elastic Net instead of Lasso or Ridge when:\n",
        "\n",
        "\n",
        "\n",
        " 1. You Have Many Correlated Features\n",
        "\n",
        "* Lasso tends to pick only one feature from a group of correlated features and ignores the others.\n",
        "* Elastic Net can include groups of correlated variables in the model, which is often more appropriate.\n",
        "\n",
        "\n",
        "\n",
        " 2. You Need Both Feature Selection and Coefficient Shrinkage\n",
        "\n",
        "* Ridge keeps all variables (no feature selection), while Lasso performs variable selection but may be unstable.\n",
        "* Elastic Net combines both L1 and L2 penalties:\n",
        "\n",
        "  * Selects important features (like Lasso).\n",
        "  * Shrinks coefficients (like Ridge) to stabilize the model.\n",
        "\n",
        "\n",
        "\n",
        " 3. When Lasso is Too Aggressive or Unstable\n",
        "\n",
        "* In high-dimensional data (more features than samples), Lasso can behave erratically, dropping too many features or becoming sensitive to data noise.\n",
        "* Elastic Net balances this by using the L2 component to add robustness.\n",
        "\n",
        "\n",
        "\n",
        " 4. When the Number of Predictors Is Much Greater Than the Number of Observations (p >> n)\n",
        "\n",
        "* Lasso may select at most n variables before it saturates.\n",
        "* Elastic Net does not have this limitation and can handle high-dimensional datasets more effectively.\n",
        "\n",
        "\n",
        "\n",
        " In Summary:\n",
        "\n",
        "Use Elastic Net when:\n",
        "\n",
        "* Your predictors are highly correlated.\n",
        "* You want feature selection + coefficient shrinkage.\n",
        "* You're working with high-dimensional data.\n",
        "* Lasso or Ridge alone underperform or overfit.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E5YaFEQuhPDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.8  What is the impact of the regularization parameter (λ) in Logistic Regression?'''"
      ],
      "metadata": {
        "id": "LzfJtvBkhcE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The regularization parameter (λ) in logistic regression controls the strength of the penalty applied to the model's coefficients during training. Its value directly affects the complexity, performance, and generalization of the model.\n",
        "\n",
        "\n",
        "\n",
        " Impact of Regularization Parameter (λ):\n",
        "\n",
        "1. When λ is very large (high regularization):\n",
        "\n",
        "* The penalty term becomes dominant.\n",
        "* Coefficients are heavily shrunk toward zero.\n",
        "* The model becomes simpler, with less variance.\n",
        "* May lead to underfitting (too simple to capture patterns in the data).\n",
        "\n",
        "2. When λ is very small (low regularization):\n",
        "\n",
        "* The penalty is minimal.\n",
        "* Coefficients can take large values to fit the training data.\n",
        "* The model becomes more flexible with higher variance.\n",
        "* May lead to overfitting (captures noise in the training data).\n",
        "\n",
        "\n",
        "\n",
        " Trade-off:\n",
        "\n",
        "* High λ: Low variance, high bias → Underfitting.\n",
        "* Low λ: High variance, low bias → Overfitting.\n",
        "\n",
        "\n",
        "\n",
        " In scikit-learn (`LogisticRegression`):\n",
        "\n",
        "* The parameter is `C`, which is inverse of λ:\n",
        "\n",
        "  $$\n",
        "  C = \\frac{1}{\\lambda}\n",
        "  $$\n",
        "\n",
        "  * Large C → Less regularization (like small λ).\n",
        "  * Small C → More regularization (like large λ).\n",
        "\n",
        "\n",
        "\n",
        " Summary:\n",
        "\n",
        "The regularization parameter λ balances model complexity and performance:\n",
        "\n",
        "* Tune λ (or C) using cross-validation to find the best value for your dataset.\n",
        "* A well-chosen λ can significantly improve generalization on unseen data.\n"
      ],
      "metadata": {
        "id": "q4lgKX40hi2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.9 What are the key assumptions of Logistic Regression?'''"
      ],
      "metadata": {
        "id": "oRPlIrNMiQzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Binary or Categorical Dependent Variable\n",
        "\n",
        "* The dependent variable should be binary (e.g., 0 or 1) for binary logistic regression.\n",
        "* Multinomial or ordinal logistic regression can be used for more than two classes.\n",
        "\n",
        "\n",
        "\n",
        " 2. Linear Relationship Between Independent Variables and Logit\n",
        "\n",
        "* Logistic regression assumes a linear relationship between the independent variables and the log odds (logit) of the dependent variable:\n",
        "\n",
        "  $$\n",
        "  \\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_nX_n\n",
        "  $$\n",
        "\n",
        "\n",
        "\n",
        " 3. No or Little Multicollinearity\n",
        "\n",
        "* Independent variables should not be highly correlated with each other.\n",
        "* High multicollinearity can make estimates unreliable.\n",
        "* Use Variance Inflation Factor (VIF) to detect multicollinearity.\n",
        "\n",
        "\n",
        "\n",
        " 4. Independence of Observations\n",
        "\n",
        "* Observations should be independent of each other.\n",
        "* Violations (e.g., time series or clustered data) require specialized models (e.g., mixed-effects logistic regression).\n",
        "\n",
        "\n",
        "\n",
        " 5. Large Sample Size\n",
        "\n",
        "* Logistic regression requires a relatively large sample size, especially when the event of interest is rare.\n",
        "* This ensures stable and reliable estimates.\n",
        "\n",
        "\n",
        "\n",
        " 6. No Extreme Outliers\n",
        "\n",
        "* While logistic regression is more robust than linear regression, extreme outliers in independent variables can still distort the model.\n",
        "\n",
        "\n",
        "\n",
        " 7. Independent Variables Can Be Continuous or Categorical\n",
        "\n",
        "* Logistic regression can handle both types, but categorical variables must be encoded (e.g., one-hot encoding).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YTfjMaDLigvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.10 What are some alternatives to Logistic Regression for classification tasks?'''"
      ],
      "metadata": {
        "id": "ulNq8qKci2x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Decision Tree Classifier\n",
        "\n",
        "* Tree-based model that splits the data based on feature values.\n",
        "* Pros: Easy to interpret, no need for feature scaling.\n",
        "* Cons: Prone to overfitting.\n",
        "\n",
        "\n",
        "\n",
        " 2. Random Forest\n",
        "\n",
        "* An ensemble of decision trees using bagging (bootstrap aggregation).\n",
        "* Pros: Reduces overfitting, handles non-linearities well.\n",
        "* Cons: Less interpretable than a single tree.\n",
        "\n",
        "\n",
        "\n",
        " 3. Support Vector Machine (SVM)\n",
        "\n",
        "* Finds the optimal hyperplane to separate classes.\n",
        "* Pros: Effective in high-dimensional spaces.\n",
        "* Cons: Not ideal for large datasets, sensitive to kernel and parameters.\n",
        "\n",
        "\n",
        "\n",
        " 4. K-Nearest Neighbors (KNN)\n",
        "\n",
        "* Classifies based on the majority class of nearest neighbors.\n",
        "* Pros: Simple, no training phase.\n",
        "* Cons: Computationally expensive at prediction time, sensitive to irrelevant features.\n",
        "\n",
        "\n",
        "\n",
        " 5. Naive Bayes\n",
        "\n",
        "* Probabilistic classifier based on Bayes’ Theorem with strong independence assumptions.\n",
        "* Pros: Fast, works well with text data (e.g., spam detection).\n",
        "* Cons: Assumes feature independence, which may not hold true.\n",
        "\n",
        "\n",
        "\n",
        " 6. Gradient Boosting Machines (e.g., XGBoost, LightGBM, CatBoost)\n",
        "\n",
        "* Builds trees sequentially to correct previous errors.\n",
        "* Pros: High accuracy, handles mixed data types.\n",
        "* Cons: Requires tuning, longer training time.\n",
        "\n",
        "\n",
        "\n",
        " 7. Neural Networks\n",
        "\n",
        "* Multi-layer models that can learn complex patterns.\n",
        "* Pros: Powerful for large datasets and non-linear data.\n",
        "* Cons: Requires more data and computational power; less interpretable.\n",
        "\n",
        "\n",
        "\n",
        " 8. Linear Discriminant Analysis (LDA)\n",
        "\n",
        "* Projects features to a lower-dimensional space while preserving class separability.\n",
        "* Pros: Works well when class distributions are Gaussian.\n",
        "* Cons: Assumes equal class covariance matrices.\n",
        "\n",
        "\n",
        "\n",
        " 9. Quadratic Discriminant Analysis (QDA)\n",
        "\n",
        "* Similar to LDA but allows different covariance matrices.\n",
        "* Pros: More flexible than LDA.\n",
        "* Cons: Requires more data due to additional parameters.\n",
        "\n",
        "\n",
        "\n",
        " Model Selection Tip:\n",
        "\n",
        "| Scenario                                       | Suggested Model     |\n",
        "| - | - |\n",
        "| Small dataset, linear boundaries               | Logistic Regression |\n",
        "| Non-linear boundaries, interpretability needed | Decision Tree       |\n",
        "| High-dimensional data                          | SVM                 |\n",
        "| Text classification                            | Naive Bayes         |\n",
        "| Accuracy-focused, tabular data                 | XGBoost / LightGBM  |\n",
        "| Complex patterns, large dataset                | Neural Network      |\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ihFKqWRkjXXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.11 What are Classification Evaluation Metrics?'''"
      ],
      "metadata": {
        "id": "WtFb06t1j82N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification Evaluation Metrics are used to assess the performance of a classification model. They help determine how well the model is predicting the correct classes. Below are the key metrics:\n",
        "\n",
        "\n",
        "\n",
        " 1. Accuracy\n",
        "\n",
        "* Measures the overall correctness of the model.\n",
        "* Formula:\n",
        "  (True Positives + True Negatives) / Total Predictions\n",
        "* When to use: When classes are balanced.\n",
        "\n",
        "\n",
        "\n",
        " 2. Precision\n",
        "\n",
        "* Measures how many predicted positives are actually correct.\n",
        "* Formula:\n",
        "  True Positives / (True Positives + False Positives)\n",
        "* When to use: When false positives are costly (e.g., spam detection).\n",
        "\n",
        "\n",
        "\n",
        " 3. Recall (Sensitivity or True Positive Rate)\n",
        "\n",
        "* Measures how many actual positives were correctly predicted.\n",
        "* Formula:\n",
        "  True Positives / (True Positives + False Negatives)\n",
        "* When to use: When false negatives are costly (e.g., cancer detection).\n",
        "\n",
        "\n",
        "\n",
        " 4. F1-Score\n",
        "\n",
        "* Harmonic mean of precision and recall.\n",
        "* Formula:\n",
        "  2 \\* (Precision \\* Recall) / (Precision + Recall)\n",
        "* When to use: When both false positives and false negatives are important.\n",
        "\n",
        "\n",
        "\n",
        " 5. Specificity (True Negative Rate)\n",
        "\n",
        "* Measures how well the model identifies actual negatives.\n",
        "* Formula:\n",
        "  True Negatives / (True Negatives + False Positives)\n",
        "\n",
        "\n",
        "\n",
        " 6. Confusion Matrix\n",
        "\n",
        "* A table showing correct and incorrect predictions.\n",
        "* Rows: Actual classes\n",
        "* Columns: Predicted classes\n",
        "* Used to compute all other metrics.\n",
        "\n",
        "\n",
        "\n",
        " 7. ROC Curve (Receiver Operating Characteristic)\n",
        "\n",
        "* Plots True Positive Rate vs. False Positive Rate at different thresholds.\n",
        "* Helps visualize model performance.\n",
        "\n",
        "\n",
        "\n",
        " 8. AUC (Area Under the ROC Curve)\n",
        "\n",
        "* Represents the overall ability of the model to discriminate between classes.\n",
        "* Closer to 1 means better performance.\n",
        "\n",
        "\n",
        "\n",
        " 9. Log Loss (Logarithmic Loss)\n",
        "\n",
        "* Penalizes false classifications more heavily.\n",
        "* Lower values indicate better performance.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gQOQC0nqkKba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.12  How does class imbalance affect Logistic Regression?'''"
      ],
      "metadata": {
        "id": "Jx-kuOAqkgLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class imbalance significantly affects the performance of Logistic Regression and other classification models. Here's a detailed explanation:\n",
        "\n",
        "\n",
        "\n",
        " What is Class Imbalance?\n",
        "\n",
        "Class imbalance occurs when one class significantly outnumbers the other(s). For example, in a dataset for fraud detection:\n",
        "\n",
        "* Class 0 (non-fraud): 98%\n",
        "* Class 1 (fraud): 2%\n",
        "\n",
        "\n",
        "\n",
        " How It Affects Logistic Regression\n",
        "\n",
        "1. Biased Predictions\n",
        "\n",
        "   * Logistic Regression tends to favor the majority class because it minimizes overall error.\n",
        "   * The model might predict all samples as the majority class to maximize accuracy.\n",
        "\n",
        "2. Misleading Accuracy\n",
        "\n",
        "   * A high accuracy may be misleading. For example, if 98% of the data is class 0, a model predicting everything as class 0 will still have 98% accuracy but 0% recall for the minority class.\n",
        "\n",
        "3. Poor Recall for Minority Class\n",
        "\n",
        "   * The model may fail to detect important minority class instances (e.g., fraud, disease, defects).\n",
        "\n",
        "4. Skewed Probability Threshold\n",
        "\n",
        "   * The default decision threshold (0.5) may not be optimal for imbalanced data and often needs adjustment.\n",
        "\n",
        "\n",
        "\n",
        " How to Handle Class Imbalance in Logistic Regression\n",
        "\n",
        "1. Resampling Techniques\n",
        "\n",
        "   * Oversampling the minority class (e.g., SMOTE)\n",
        "   * Undersampling the majority class\n",
        "\n",
        "2. Class Weights\n",
        "\n",
        "   * Use `class_weight='balanced'` in logistic regression (e.g., in scikit-learn)\n",
        "   * Penalizes misclassification of the minority class more heavily\n",
        "\n",
        "   ```python\n",
        "   from sklearn.linear_model import LogisticRegression\n",
        "   model = LogisticRegression(class_weight='balanced')\n",
        "   ```\n",
        "\n",
        "3. Evaluation Metrics\n",
        "\n",
        "   * Use metrics like Precision, Recall, F1-score, AUC-ROC instead of just accuracy.\n",
        "\n",
        "4. Adjusting Decision Threshold\n",
        "\n",
        "   * Tune the classification threshold to improve recall or precision for the minority class.\n",
        "\n",
        "5. Ensemble Methods\n",
        "\n",
        "   * Combine Logistic Regression with boosting techniques like AdaBoost or XGBoost, which handle imbalance better.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nLSWQNVrkvcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.13 What is Hyperparameter Tuning in Logistic Regression?'''"
      ],
      "metadata": {
        "id": "fb1-aKS2k-K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Tuning in Logistic Regression refers to the process of finding the best set of hyperparameters (settings) that optimize the model's performance on unseen data.\n",
        "\n",
        "What Are Hyperparameters in Logistic Regression?\n",
        "\n",
        "Unlike model parameters (like weights and bias, which are learned from data), hyperparameters are set before training and control the learning process.\n",
        "\n",
        "Key hyperparameters in Logistic Regression (especially in scikit-learn) include:\n",
        "\n",
        "| Hyperparameter | Description                                                                    |\n",
        "| -------------- | ------------------------------------------------------------------------------ |\n",
        "| `C`            | Inverse of regularization strength (smaller `C` means stronger regularization) |\n",
        "| `penalty`      | Type of regularization (`l1`, `l2`, `elasticnet`, or `none`)                   |\n",
        "| `solver`       | Optimization algorithm (`liblinear`, `saga`, `lbfgs`, etc.)                    |\n",
        "| `max_iter`     | Maximum number of iterations for convergence                                   |\n",
        "| `class_weight` | Adjusts weights for imbalanced classes                                         |\n",
        "\n",
        "Why Hyperparameter Tuning Is Important\n",
        "\n",
        "- Prevents overfitting or underfitting\n",
        "\n",
        "- Improves accuracy, recall, F1-score, or other metrics\n",
        "\n",
        "- Helps deal with class imbalance, regularization, and convergence issues\n",
        "\n",
        "Common Hyperparameter Tuning Methods\n",
        "1. Grid Search\n",
        "\n",
        "- Tests all combinations of specified values"
      ],
      "metadata": {
        "id": "5C1-H8Z2lDPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "model = LogisticRegression()\n",
        "grid = GridSearchCV(model, param_grid, cv=5, scoring='f1')\n",
        "grid.fit(X_train, y_train)\n",
        "```"
      ],
      "metadata": {
        "id": "onmzOrYom4W6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Random Search\n",
        "\n",
        "- Randomly samples combinations of hyperparameters"
      ],
      "metadata": {
        "id": "82oRtKaHl4TD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "random = RandomizedSearchCV(model, param_grid, n_iter=10, cv=5, scoring='f1')\n",
        "random.fit(X_train, y_train)\n",
        "```"
      ],
      "metadata": {
        "id": "mz1OgHQ_mgWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Bayesian Optimization (advanced)\n",
        "\n",
        "- Uses probabilistic models to choose the best hyperparameters efficiently (e.g., with Optuna or scikit-optimize)\n",
        "\n",
        "Example: Tuning Regularization Parameter C\n",
        "\n",
        "- High C (e.g., 10) → Less regularization → Model may overfit\n",
        "\n",
        "- Low C (e.g., 0.01) → More regularization → Model may underfit\n",
        "\n",
        "- Ideal C balances bias and variance\n",
        "\n"
      ],
      "metadata": {
        "id": "Ol52RqyPmOFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.14 What are different solvers in Logistic Regression? Which one should be used?'''"
      ],
      "metadata": {
        "id": "-KEyDDIpnDAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Logistic Regression, a solver is the algorithm used to optimize the cost function. Each solver has different strengths based on data size, regularization type, and sparsity.\n",
        "\n",
        "List of Solvers in scikit-learn Logistic Regression\n",
        "\n",
        "| Solver        | Description                                                                     | Supports L1? | Supports L2? | Multiclass?       | Works for large datasets? |\n",
        "| ------------- | ------------------------------------------------------------------------------- | ------------ | ------------ | ----------------- | ------------------------- |\n",
        "| liblinear | Good for small datasets, supports L1 & L2 regularization                        | Yes          | Yes          | One-vs-Rest       | No                        |\n",
        "| lbfgs     | Fast and robust, uses quasi-Newton method; good for multiclass                  | No           | Yes          | Multinomial & OvR | Yes                       |\n",
        "| sag       | Stochastic Average Gradient descent; faster on large datasets (n > 10k)         | No           | Yes          | Multinomial & OvR | Yes                       |\n",
        "| saga      | Extension of sag; supports L1, L2, and Elastic Net; works well with sparse data | Yes          | Yes          | Multinomial & OvR | Yes                       |\n",
        "| newton-cg | Uses Newton’s method; works well for L2 and multiclass classification           | No           | Yes          | Multinomial & OvR | Yes                       |\n",
        "\n",
        "Which Solver to Use?\n",
        "\n",
        "* liblinear\n",
        "\n",
        "  * Best for small datasets\n",
        "  * Use when L1 regularization is needed\n",
        "  * Default for binary classification if no solver is specified\n",
        "\n",
        "* lbfgs\n",
        "\n",
        "  * Recommended for multiclass problems\n",
        "  * Works well with L2 regularization\n",
        "  * Default for many logistic regression tasks\n",
        "\n",
        "* sag\n",
        "\n",
        "  * Good for large datasets\n",
        "  * Requires L2 regularization\n",
        "\n",
        "* saga\n",
        "\n",
        "  * Best for large, sparse datasets\n",
        "  * Only one supporting L1, L2, and ElasticNet\n",
        "  * Useful for online learning\n",
        "\n",
        "* newton-cg\n",
        "\n",
        "  * Suitable for multiclass problems\n",
        "  * Slower but can be more accurate\n",
        "\n",
        "Quick Recommendation Table\n",
        "\n",
        "| Situation                             | Recommended Solver |\n",
        "| ------------------------------------- | ------------------ |\n",
        "| Small dataset                         | liblinear          |\n",
        "| Large dataset                         | saga or sag        |\n",
        "| Multiclass classification             | lbfgs or saga      |\n",
        "| Need L1 or Elastic Net regularization | liblinear or saga  |\n",
        "| Sparse features                       | saga               |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "39DT36CcnJyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.15 How is Logistic Regression extended for multiclass classification?'''"
      ],
      "metadata": {
        "id": "7dMccMkinohG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression is naturally a binary classifier (i.e., it predicts two classes), but it can be extended to handle multiclass classification using the following approaches:\n",
        "\n",
        "\n",
        " 1. One-vs-Rest (OvR) or One-vs-All\n",
        "\n",
        "* How it works:\n",
        "\n",
        "  * For K classes, train K separate binary classifiers.\n",
        "  * Each classifier distinguishes one class vs. the rest.\n",
        "  * For prediction, choose the class whose classifier gives the highest probability.\n",
        "\n",
        "* Used by: `liblinear`, `saga`, `newton-cg` (by default unless otherwise specified)\n",
        "\n",
        "* Pros: Simple, interpretable\n",
        "\n",
        "* Cons: May lead to ambiguous predictions if multiple classifiers have similar confidence scores.\n",
        "\n",
        "\n",
        "\n",
        " 2. Multinomial (Softmax Regression)\n",
        "\n",
        "* How it works:\n",
        "\n",
        "  * Trains a single model that predicts the probability of each class using the softmax function.\n",
        "  * Generalizes the sigmoid function to multiple classes.\n",
        "\n",
        "* Cost function: Cross-entropy loss for all classes simultaneously.\n",
        "\n",
        "* Used by: `lbfgs`, `saga`, `newton-cg` (when `multi_class='multinomial'` is specified)\n",
        "\n",
        "* Pros: Considers all classes at once; more accurate for mutually exclusive classes\n",
        "\n",
        "* Cons: More computationally expensive than OvR\n",
        "\n",
        "\n",
        "\n",
        " Example in scikit-learn:\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Multinomial Logistic Regression\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)\n",
        "model.fit(X, y)\n",
        "\n",
        "print(\"Classes:\", model.classes_)\n",
        "print(\"Predicted:\", model.predict(X[:5]))\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        " Summary\n",
        "\n",
        "| Approach    | Method                     | Suitable Solvers       | Best For                      |\n",
        "| ----------- | -------------------------- | ---------------------- | ----------------------------- |\n",
        "| One-vs-Rest | K binary classifiers       | liblinear, saga        | Simpler datasets              |\n",
        "| Multinomial | Single softmax-based model | lbfgs, saga, newton-cg | More complex, multiclass data |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j582MB3on6G3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.16  What are the advantages and disadvantages of Logistic Regression?'''"
      ],
      "metadata": {
        "id": "4M-8sbgGodp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        " Advantages of Logistic Regression\n",
        "\n",
        "1. Simple and easy to implement\n",
        "\n",
        "   * Logistic Regression is easy to understand, interpret, and implement, making it suitable for baseline models.\n",
        "\n",
        "2. Fast training and prediction\n",
        "\n",
        "   * It has low computational complexity and trains quickly, even on large datasets.\n",
        "\n",
        "3. Probabilistic interpretation\n",
        "\n",
        "   * Outputs probabilities that can be useful for decision-making (e.g., in medical diagnosis or risk assessment).\n",
        "\n",
        "4. Less prone to overfitting with regularization\n",
        "\n",
        "   * L1 (Lasso) and L2 (Ridge) regularization help prevent overfitting.\n",
        "\n",
        "5. Works well with linearly separable data\n",
        "\n",
        "   * Provides good performance when classes are linearly separable.\n",
        "\n",
        "6. Feature importance\n",
        "\n",
        "   * Coefficients help in understanding the effect of each feature on the prediction.\n",
        "\n",
        "\n",
        "\n",
        " Disadvantages of Logistic Regression\n",
        "\n",
        "1. Assumes linear relationship\n",
        "\n",
        "   * Assumes a linear relationship between the independent variables and the log-odds of the target variable, which may not always hold true.\n",
        "\n",
        "2. Not suitable for complex relationships\n",
        "\n",
        "   * Fails to perform well if the decision boundary is non-linear or complex.\n",
        "\n",
        "3. Sensitive to outliers\n",
        "\n",
        "   * Outliers can significantly affect model performance if not handled properly.\n",
        "\n",
        "4. Requires little or no multicollinearity\n",
        "\n",
        "   * High correlation among features (multicollinearity) can distort the coefficients and reduce model interpretability.\n",
        "\n",
        "5. Limited to binary or multinomial classification\n",
        "\n",
        "   * Cannot naturally handle ordinal or hierarchical classes without additional modeling.\n",
        "\n",
        "6. Needs large sample size\n",
        "\n",
        "   * Performs poorly with very small datasets or highly imbalanced data without resampling or class-weight adjustments.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cTJvc_cboolK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.17 What are some use cases of Logistic Regression?'''"
      ],
      "metadata": {
        "id": "1zHCbiYCpEEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some common and important use cases of Logistic Regression across various industries:\n",
        "\n",
        "\n",
        "\n",
        " 1. Medical Diagnosis\n",
        "\n",
        "* Disease prediction: Predict whether a patient has a certain disease (e.g., diabetes, heart disease, cancer) based on medical parameters.\n",
        "* Outcome prediction: Estimate the probability of survival or recovery after treatment.\n",
        "\n",
        "\n",
        "\n",
        " 2. Marketing and Sales\n",
        "\n",
        "* Customer churn prediction: Determine whether a customer is likely to stop using a service.\n",
        "* Lead conversion: Predict whether a marketing lead will convert into a paying customer.\n",
        "* Response modeling: Identify if a customer will respond to a marketing campaign.\n",
        "\n",
        "\n",
        "\n",
        " 3. Banking and Finance\n",
        "\n",
        "* Credit scoring: Assess the probability that a loan applicant will default on a loan.\n",
        "* Fraud detection: Classify whether a transaction is fraudulent or not.\n",
        "\n",
        "\n",
        "\n",
        " 4. Human Resources\n",
        "\n",
        "* Employee attrition: Predict whether an employee is likely to leave the company.\n",
        "* Hiring decisions: Predict if a candidate will be a good fit based on their profile.\n",
        "\n",
        "\n",
        "\n",
        " 5. E-commerce and Retail\n",
        "\n",
        "* Purchase prediction: Predict if a user will buy a product after viewing it.\n",
        "* Click-through rate prediction: Estimate whether a user will click on an ad.\n",
        "\n",
        "\n",
        "\n",
        " 6. Political Science\n",
        "\n",
        "* Voter prediction: Predict whether a person will vote for a certain party based on demographics and survey data.\n",
        "\n",
        "\n",
        "\n",
        " 7. Manufacturing\n",
        "\n",
        "* Product defect detection: Predict whether a product is defective based on input parameters.\n",
        "* Maintenance prediction: Predict if a machine will fail soon (binary classification: fail or not fail).\n",
        "\n",
        "\n",
        "\n",
        " 8. Education\n",
        "\n",
        "* Student dropout prediction: Predict whether a student will drop out based on academic and personal data.\n",
        "* Admission prediction: Estimate the likelihood of a student being admitted to a program.\n",
        "\n"
      ],
      "metadata": {
        "id": "ME6ma2QGpRs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.18 What is the difference between Softmax Regression and Logistic Regression?'''"
      ],
      "metadata": {
        "id": "jhpedrPppx6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difference Between Softmax Regression and Logistic Regression\n",
        "\n",
        "| Aspect                     | Logistic Regression                              | Softmax Regression (Multinomial Logistic Regression)                   |         |                                                                             |\n",
        "| -------------------------- | ---------------------------------------------------- | -------------------------------------------------------------------------- | ------- | --------------------------------------------------------------------------- |\n",
        "| Type of Classification | Binary Classification (2 classes)                    | Multiclass Classification (3 or more classes)                              |         |                                                                             |\n",
        "| Output                 | Probability of class 1 (and by subtraction, class 0) | Probability distribution across all classes                                |         |                                                                             |\n",
        "| Activation Function    | Sigmoid Function                                     | Softmax Function                                                           |         |                                                                             |\n",
        "| Equation               | ( P(y=1                                              | x) = \\frac{1}{1 + e^{-(\\beta\\_0 + \\beta\\_1x\\_1 + \\dots + \\beta\\_nx\\_n)}} ) | ( P(y=i | x) = \\frac{e^{\\beta\\_i^T x}}{\\sum\\_{j=1}^K e^{\\beta\\_j^T x}} ), for class i |\n",
        "| Decision Boundary      | Linear (in binary space)                             | Linear boundaries between multiple classes                                 |         |                                                                             |\n",
        "| Loss Function          | Binary Cross-Entropy (Log Loss)                      | Categorical Cross-Entropy (Multinomial Log Loss)                           |         |                                                                             |\n",
        "| Used When              | When the target has two categories (e.g., 0 or 1)    | When the target has more than two categories (e.g., 0, 1, 2, ..., k)       |         |                                                                             |\n",
        "| Model Complexity       | Simpler                                              | More complex (one set of weights per class)                                |         |                                                                             |\n",
        "\n",
        "\n",
        "\n",
        "Summary\n",
        "\n",
        "* Use Logistic Regression when you're solving a binary classification problem.\n",
        "* Use Softmax Regression when you need to handle multiclass classification problems (where a sample can belong to only one of three or more classes).\n"
      ],
      "metadata": {
        "id": "TK8L92PvqMRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.19 How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?'''"
      ],
      "metadata": {
        "id": "6j_8uPIgqaZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Choosing Between One-vs-Rest (OvR) and Softmax for Multiclass Classification\n",
        "\n",
        "When performing multiclass classification, both One-vs-Rest (OvR) and Softmax (Multinomial Logistic Regression) can be used. Here's how to choose between them:\n",
        "\n",
        "\n",
        "\n",
        "1. Understanding the Methods\n",
        "\n",
        "| Method                                        | Description                                                                                                                                                   |\n",
        "| --------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| One-vs-Rest (OvR)                         | Trains one binary classifier per class. Each classifier separates one class from all others. Final prediction is the class with the highest confidence score. |\n",
        "| Softmax (Multinomial Logistic Regression) | Trains a single model that directly predicts probabilities for all classes simultaneously using the softmax function.                                         |\n",
        "\n",
        "\n",
        "\n",
        "2. Comparison Table\n",
        "\n",
        "| Criteria                | One-vs-Rest (OvR)                                  | Softmax (Multinomial Logistic Regression)                                              |\n",
        "| ----------------------- | -------------------------------------------------- | -------------------------------------------------------------------------------------- |\n",
        "| Number of Models    | One per class (K binary classifiers)               | One unified model                                                                      |\n",
        "| Training Complexity | Simpler to train each classifier separately        | Slightly more complex (joint optimization)                                             |\n",
        "| Prediction Speed    | Slower (runs multiple classifiers)                 | Faster (single pass)                                                                   |\n",
        "| Performance         | Good if classes are well-separated                 | Better for correlated or overlapping classes                                           |\n",
        "| Interpretability    | Easier to interpret each classifier                | Slightly harder to interpret                                                           |\n",
        "| Libraries Support   | Widely supported (e.g., `scikit-learn`)            | Supported (e.g., `sklearn.linear_model.LogisticRegression(multi_class='multinomial')`) |\n",
        "| When to Use         | Small number of classes, imbalance between classes | Many classes or complex decision boundaries                                            |\n",
        "\n",
        "\n",
        "3. When to Use Which\n",
        "\n",
        "* Use OvR if:\n",
        "\n",
        "  * You have a small number of classes.\n",
        "  * Your data is imbalanced.\n",
        "  * You want better interpretability for each class individually.\n",
        "\n",
        "* Use Softmax if:\n",
        "\n",
        "  * You have many classes.\n",
        "  * Classes are mutually exclusive and not well separated.\n",
        "  * You need better overall model coherence and performance.\n",
        "\n",
        "\n",
        "\n",
        "4. Example in scikit-learn\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# One-vs-Rest (default)\n",
        "model_ovr = LogisticRegression(multi_class='ovr')\n",
        "\n",
        "# Softmax (Multinomial)\n",
        "model_softmax = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9hXyJskBqo16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.20 How do we interpret coefficients in Logistic Regression?'''"
      ],
      "metadata": {
        "id": "ixCrSemmsCJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Interpretation of Coefficients in Logistic Regression\n",
        "\n",
        "In Logistic Regression, the coefficients represent the log-odds of the outcome. Here's how to interpret them:\n",
        "\n",
        "\n",
        "\n",
        " 1. Logistic Regression Equation\n",
        "\n",
        "For binary classification, the logistic regression model is:\n",
        "\n",
        "$$\n",
        "\\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $p$ is the probability of the positive class\n",
        "* $\\beta_0$ is the intercept\n",
        "* $\\beta_i$ are the feature coefficients\n",
        "* $x_i$ are the feature values\n",
        "\n",
        "\n",
        "\n",
        " 2. Interpretation of Coefficients (β)\n",
        "\n",
        "* Each coefficient $\\beta_i$ represents the change in the log-odds of the outcome for a one-unit increase in feature $x_i$, holding all other variables constant.\n",
        "\n",
        "* If $\\beta_i > 0$: As $x_i$ increases, the log-odds of the positive class increase → probability of the positive class increases.\n",
        "\n",
        "* If $\\beta_i < 0$: As $x_i$ increases, the log-odds of the positive class decrease → probability of the positive class decreases.\n",
        "\n",
        "\n",
        "\n",
        " 3. From Log-Odds to Odds Ratio\n",
        "\n",
        "To make interpretation easier, we convert the log-odds to odds ratio:\n",
        "\n",
        "$$\n",
        "\\text{Odds Ratio} = e^{\\beta_i}\n",
        "$$\n",
        "\n",
        "* If $e^{\\beta_i} > 1$: A one-unit increase in $x_i$ increases the odds of the positive class.\n",
        "* If $e^{\\beta_i} < 1$: A one-unit increase in $x_i$ decreases the odds.\n",
        "* If $e^{\\beta_i} = 1$: No effect on the odds.\n",
        "\n",
        "\n",
        "\n",
        " 4. Example\n",
        "\n",
        "Suppose a logistic regression outputs:\n",
        "\n",
        "* $\\beta_1 = 0.693$\n",
        "\n",
        "Then:\n",
        "\n",
        "* Log-odds increase by 0.693 for a one-unit increase in $x_1$\n",
        "* Odds ratio = $e^{0.693} ≈ 2$\n",
        "* Interpretation: A one-unit increase in $x_1$ doubles the odds of the outcome being in the positive class.\n",
        "\n",
        "\n",
        "\n",
        " 5. For Categorical Variables\n",
        "\n",
        "* Categorical variables (after encoding) compare the effect of one category vs the reference category.\n",
        "* Coefficients show how the presence of a category affects the odds compared to the baseline.\n",
        "\n"
      ],
      "metadata": {
        "id": "qPms34ppskMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRACTICAL"
      ],
      "metadata": {
        "id": "3KgPBCZvs-ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.1 Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy'''"
      ],
      "metadata": {
        "id": "a_c8BsmstEeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Logistic Regression Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7nxmQR5tPve",
        "outputId": "df3d4db7-9734-4769-8d7a-f454dc7cbe39"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.2  Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy'''"
      ],
      "metadata": {
        "id": "GJjPrFC8tiMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize features (important for L1 regularization)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with L1 regularization\n",
        "model = LogisticRegression(penalty='l1', solver='saga', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"L1-Regularized Logistic Regression Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xaH1vZotnG7",
        "outputId": "2a183bf4-43f1-4bdb-f2ad-d183ae6a1fc2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1-Regularized Logistic Regression Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.3  Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients'''"
      ],
      "metadata": {
        "id": "dd99UdYFt2DC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the feature set\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print model accuracy\n",
        "print(\"L2-Regularized Logistic Regression Accuracy:\", accuracy)\n",
        "\n",
        "# Print model coefficients and intercepts\n",
        "print(\"\\nModel Coefficients:\\n\", model.coef_)\n",
        "print(\"\\nIntercepts:\\n\", model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvAisGJjt78f",
        "outputId": "0dfa7472-5fda-421e-d6f3-79d67d0b7d03"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2-Regularized Logistic Regression Accuracy: 1.0\n",
            "\n",
            "Model Coefficients:\n",
            " [[-1.02102589  1.1315509  -1.81471682 -1.68763103]\n",
            " [ 0.53439559 -0.28357112 -0.34273213 -0.73103351]\n",
            " [ 0.4866303  -0.84797979  2.15744895  2.41866455]]\n",
            "\n",
            "Intercepts:\n",
            " [-0.24853241  1.97284408 -1.72431167]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " '''Q.4 Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')'''"
      ],
      "metadata": {
        "id": "4iAoB8UAuHD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train Logistic Regression model with Elastic Net regularization\n",
        "model = LogisticRegression(\n",
        "    penalty='elasticnet',\n",
        "    solver='saga',            # Required for elasticnet\n",
        "    l1_ratio=0.5,             # Balance between L1 and L2 (0 = L2, 1 = L1)\n",
        "    max_iter=1000,\n",
        "    multi_class='multinomial' # For multiclass classification\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Elastic Net Logistic Regression Accuracy:\", accuracy)\n",
        "\n",
        "# Print coefficients and intercepts\n",
        "print(\"\\nModel Coefficients:\\n\", model.coef_)\n",
        "print(\"\\nModel Intercepts:\\n\", model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efbD3bwfue6X",
        "outputId": "675ec012-3f7c-4a22-bafb-cd4b29a91997"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elastic Net Logistic Regression Accuracy: 1.0\n",
            "\n",
            "Model Coefficients:\n",
            " [[-0.82214374  1.17435549 -1.96499739 -1.70922937]\n",
            " [ 0.0799724   0.          0.         -0.3723561 ]\n",
            " [ 0.         -0.65344297  2.74171511  3.06929649]]\n",
            "\n",
            "Model Intercepts:\n",
            " [-0.20109765  2.12558023 -1.92448259]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.5 Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr' '''"
      ],
      "metadata": {
        "id": "34XKrq1zuhoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression with One-vs-Rest (OvR) strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"OvR Logistic Regression Accuracy:\", accuracy)\n",
        "\n",
        "# Print model coefficients and intercepts\n",
        "print(\"\\nModel Coefficients:\\n\", model.coef_)\n",
        "print(\"\\nModel Intercepts:\\n\", model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IrMTse9uqWm",
        "outputId": "fafdd302-8f85-46a1-c73f-20c0cc5f6c09"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OvR Logistic Regression Accuracy: 0.9666666666666667\n",
            "\n",
            "Model Coefficients:\n",
            " [[-0.76560747  1.33826997 -1.6082868  -1.42234276]\n",
            " [ 0.26117413 -1.23992478  0.53679084 -0.73948844]\n",
            " [ 0.04512377 -0.22445255  1.75763427  2.42627487]]\n",
            "\n",
            "Model Intercepts:\n",
            " [-1.54125626 -0.82971157 -2.48825514]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.6 Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy'''"
      ],
      "metadata": {
        "id": "ZwBROfZquzUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define the model\n",
        "log_reg = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters and accuracy\n",
        "best_model = grid.best_estimator_\n",
        "best_params = grid.best_params_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Output\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Accuracy on Test Set:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D4oezlIu5I8",
        "outputId": "fca1a050-322c-45f1-9ecf-e5ee506a500a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1'}\n",
            "Best Accuracy on Test Set: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.7  Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy'''"
      ],
      "metadata": {
        "id": "PdNl-BOqvBqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Initialize Logistic Regression\n",
        "model = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
        "\n",
        "# Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Evaluate model using cross_val_score\n",
        "scores = cross_val_score(model, X_scaled, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Print accuracy for each fold and the average accuracy\n",
        "print(\"Accuracy for each fold:\", scores)\n",
        "print(\"Average Accuracy:\", np.mean(scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQulntjevMgZ",
        "outputId": "d9ae7cc1-9091-4969-db93-ead2ee0b7024"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for each fold: [0.93333333 0.96666667 0.8        0.93333333 0.86666667]\n",
            "Average Accuracy: 0.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.8 Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.'''"
      ],
      "metadata": {
        "id": "JIcm2kexvO5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```PYTHON\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset from CSV file\n",
        "# Replace 'your_file.csv' with your actual CSV filename\n",
        "df = pd.read_csv('your_file.csv')\n",
        "\n",
        "# Assume the last column is the target\n",
        "X = df.iloc[:, :-1]  # all columns except the last\n",
        "y = df.iloc[:, -1]   # the last column\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Logistic Regression Accuracy:\", accuracy)\n",
        "```"
      ],
      "metadata": {
        "id": "fXTNbHSmv3y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.9 Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy'''"
      ],
      "metadata": {
        "id": "sIgJfla4wDT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```PYTHON\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Load dataset\n",
        "# Replace with your actual CSV filename\n",
        "df = pd.read_csv(\"your_file.csv\")\n",
        "\n",
        "# Prepare features and target\n",
        "X = df.iloc[:, :-1]  # features (all columns except last)\n",
        "y = df.iloc[:, -1]   # target (last column)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Define parameter grid\n",
        "param_dist = {\n",
        "    'C': uniform(loc=0.01, scale=10),\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
        "    'solver': ['saga', 'liblinear', 'lbfgs', 'newton-cg'],\n",
        "    'l1_ratio': uniform(0, 1)  # Only used when penalty='elasticnet'\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV setup\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Results\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "```"
      ],
      "metadata": {
        "id": "K0H7o7_pwf6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.10 Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy'''"
      ],
      "metadata": {
        "id": "6TD69vYbwsev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```PYTHON\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a sample multiclass dataset (Iris)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create logistic regression model\n",
        "logreg = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='auto')\n",
        "\n",
        "# Wrap it with One-vs-One strategy\n",
        "ovo_model = OneVsOneClassifier(logreg)\n",
        "\n",
        "# Train the model\n",
        "ovo_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = ovo_model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Output results\n",
        "print(\"One-vs-One Logistic Regression Accuracy:\", accuracy)\n",
        "```"
      ],
      "metadata": {
        "id": "w0iPSDlPw82Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.11 Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification'''"
      ],
      "metadata": {
        "id": "Y76JaN42xDDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```PYTHON\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "\n",
        "# Step 1: Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3,\n",
        "                           n_redundant=0, n_classes=2, random_state=42)\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train a logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Step 6: Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Step 7: Visualize the confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Pred 0', 'Pred 1'], yticklabels=['True 0', 'True 1'])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "1WNx1IMpxgZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.12  Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score'''"
      ],
      "metadata": {
        "id": "HUKY7CPkxlYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Step 1: Generate synthetic binary classification data\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3,\n",
        "                           n_redundant=0, n_classes=2, random_state=42)\n",
        "\n",
        "# Step 2: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate model performance\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print evaluation metrics\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall:    {recall:.2f}\")\n",
        "print(f\"F1-Score:  {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnuZCckdxtE4",
        "outputId": "9c5e3b2f-cd1e-4c9a-bf99-900190fbc6b0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.93\n",
            "Recall:    0.90\n",
            "F1-Score:  0.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.13 Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance'''"
      ],
      "metadata": {
        "id": "y3TvOUclPgp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Step 1: Generate imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3,\n",
        "                           n_redundant=0, n_clusters_per_class=1,\n",
        "                           weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "# Step 2: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train logistic regression without class weights\n",
        "model_default = LogisticRegression()\n",
        "model_default.fit(X_train, y_train)\n",
        "y_pred_default = model_default.predict(X_test)\n",
        "\n",
        "# Step 4: Train logistic regression with class weights\n",
        "model_weighted = LogisticRegression(class_weight='balanced')  # Automatically adjusts weights inversely proportional to class frequencies\n",
        "model_weighted.fit(X_train, y_train)\n",
        "y_pred_weighted = model_weighted.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate both models\n",
        "print(\"Classification Report WITHOUT Class Weights:\")\n",
        "print(classification_report(y_test, y_pred_default))\n",
        "\n",
        "print(\"\\nClassification Report WITH Class Weights:\")\n",
        "print(classification_report(y_test, y_pred_weighted))\n",
        "\n",
        "# Optional: Visualize class distribution\n",
        "sns.countplot(x=y)\n",
        "plt.title(\"Class Distribution (Imbalanced Data)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "id": "PIAKEuzuPvi_",
        "outputId": "9d2e59eb-d308-4779-d796-7fd79257feed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report WITHOUT Class Weights:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.96      0.97       275\n",
            "           1       0.63      0.68      0.65        25\n",
            "\n",
            "    accuracy                           0.94       300\n",
            "   macro avg       0.80      0.82      0.81       300\n",
            "weighted avg       0.94      0.94      0.94       300\n",
            "\n",
            "\n",
            "Classification Report WITH Class Weights:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95       275\n",
            "           1       0.51      0.92      0.66        25\n",
            "\n",
            "    accuracy                           0.92       300\n",
            "   macro avg       0.75      0.92      0.81       300\n",
            "weighted avg       0.95      0.92      0.93       300\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.14 Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance'''"
      ],
      "metadata": {
        "id": "w0UeOuHsP07o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "import seaborn as sns\n",
        "\n",
        "# Step 1: Load Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "\n",
        "# Step 2: Drop columns with too many missing values or irrelevant features\n",
        "df = df.drop(columns=['deck', 'embark_town', 'alive', 'who', 'adult_male', 'class'])\n",
        "\n",
        "# Step 3: Handle missing values\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "df[['age', 'embarked']] = imputer.fit_transform(df[['age', 'embarked']])\n",
        "\n",
        "# Step 4: Encode categorical variables\n",
        "label_cols = ['sex', 'embarked', 'alone']\n",
        "le = LabelEncoder()\n",
        "for col in label_cols:\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# Step 5: Define features and target\n",
        "X = df[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'alone']]\n",
        "y = df['survived']\n",
        "\n",
        "# Step 6: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Predictions and evaluation\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaWqUR19QBd5",
        "outputId": "4e8ad98b-3b54-4f00-ad42-23241aa6415b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7988826815642458\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.83       105\n",
            "           1       0.77      0.73      0.75        74\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.79      0.79      0.79       179\n",
            "weighted avg       0.80      0.80      0.80       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.15  Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling'''"
      ],
      "metadata": {
        "id": "5OoGyoneQEUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset (binary classification)\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression WITHOUT Scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=1000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Apply Standard Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Logistic Regression WITH Scaling\n",
        "model_scaling = LogisticRegression(max_iter=1000)\n",
        "model_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = model_scaling.predict(X_test_scaled)\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "# Print Results\n",
        "print(\"Accuracy WITHOUT Scaling:\", round(accuracy_no_scaling, 4))\n",
        "print(\"Accuracy WITH Scaling   :\", round(accuracy_scaling, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPdSWDGVQQfq",
        "outputId": "99b98fc5-bf8d-491a-c0a0-77f391b9d9e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT Scaling: 0.9561\n",
            "Accuracy WITH Scaling   : 0.9737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.16 Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score'''"
      ],
      "metadata": {
        "id": "fVFcgTKsQTVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict probabilities for ROC AUC\n",
        "y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Plot ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "mfg7sWBvQYYS",
        "outputId": "9fe14eca-7ff5-4e3d-e8b3-48fb36843a3e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9974\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.17 Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy'''"
      ],
      "metadata": {
        "id": "YxXCulk4QtDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model with C=0.5\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with C=0.5: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrWvWNFLQyax",
        "outputId": "77d99680-536a-4b2d-a9ff-cca3b7fe9f39"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with C=0.5: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.18  Write a Python program to train Logistic Regression and identify important features based on model coefficients'''"
      ],
      "metadata": {
        "id": "2hI90HL4Q-ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get feature importance (coefficients)\n",
        "coefficients = model.coef_[0]\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Coefficient': coefficients,\n",
        "    'Absolute Coefficient': np.abs(coefficients)\n",
        "})\n",
        "\n",
        "# Sort features by importance\n",
        "feature_importance_sorted = feature_importance.sort_values(\n",
        "    by='Absolute Coefficient', ascending=False\n",
        ")\n",
        "\n",
        "# Display top important features\n",
        "print(\"Top Important Features Based on Coefficients:\\n\")\n",
        "print(feature_importance_sorted.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYNYkfDyREdU",
        "outputId": "de9924ca-e758-4aa5-f8e7-95ff53dd23fa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Important Features Based on Coefficients:\n",
            "\n",
            "                 Feature  Coefficient  Absolute Coefficient\n",
            "21         worst texture    -1.350606              1.350606\n",
            "10          radius error    -1.268178              1.268178\n",
            "28        worst symmetry    -1.208200              1.208200\n",
            "7    mean concave points    -1.119804              1.119804\n",
            "26       worst concavity    -0.943053              0.943053\n",
            "13            area error    -0.907186              0.907186\n",
            "20          worst radius    -0.879840              0.879840\n",
            "23            worst area    -0.841846              0.841846\n",
            "6         mean concavity    -0.801458              0.801458\n",
            "27  worst concave points    -0.778217              0.778217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.19  Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score'''"
      ],
      "metadata": {
        "id": "Xd9fCtkURLkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train logistic regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate using Cohen's Kappa Score\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWacvGgHRQkb",
        "outputId": "c39791e2-a4ec-4a36-bdef-81e9d9137bc4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.9053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.20 Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification'''"
      ],
      "metadata": {
        "id": "jhPd7ArjRY2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train logistic regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute Precision-Recall curve and Average Precision\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f'AP = {avg_precision:.2f}', color='blue')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "Pnm4p9W7RhLf",
        "outputId": "7ee5416f-f576-4eda-9ea4-9e4b35caaa1f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.21 Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy'''"
      ],
      "metadata": {
        "id": "QJzkv67bRpmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "results = {}\n",
        "\n",
        "# Train and evaluate with each solver\n",
        "for solver in solvers:\n",
        "    print(f\"Training with solver = {solver}\")\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results[solver] = acc\n",
        "    print(f\"Accuracy with {solver}: {acc:.4f}\\n\")\n",
        "\n",
        "# Summary\n",
        "print(\"Summary of Solver Accuracies:\")\n",
        "for solver, acc in results.items():\n",
        "    print(f\"{solver}: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r04JTVVRvgg",
        "outputId": "e153b034-d4a8-46d4-fc05-c4abcdb8d715"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with solver = liblinear\n",
            "Accuracy with liblinear: 0.9561\n",
            "\n",
            "Training with solver = saga\n",
            "Accuracy with saga: 0.9649\n",
            "\n",
            "Training with solver = lbfgs\n",
            "Accuracy with lbfgs: 0.9561\n",
            "\n",
            "Summary of Solver Accuracies:\n",
            "liblinear: 0.9561\n",
            "saga: 0.9649\n",
            "lbfgs: 0.9561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.22  Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)'''"
      ],
      "metadata": {
        "id": "Rz4JlHL1R3bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Load dataset (binary classification)\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate using Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPVts7TER9mi",
        "outputId": "274de395-1129-433a-f4dc-0e50bdb55df5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.9068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.25 Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling'''"
      ],
      "metadata": {
        "id": "iLLxnFuTSFOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ---- 1. Logistic Regression on Raw Data ----\n",
        "model_raw = LogisticRegression(solver='liblinear')\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "acc_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# ---- 2. Logistic Regression on Standardized Data ----\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(solver='liblinear')\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# ---- Results ----\n",
        "print(f\"Accuracy on Raw Data:         {acc_raw:.4f}\")\n",
        "print(f\"Accuracy on Standardized Data:{acc_scaled:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S98VLIoISKVF",
        "outputId": "0dc8a4e5-842d-442f-935a-a5ad41172643"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Raw Data:         0.9561\n",
            "Accuracy on Standardized Data:0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.24 Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation'''"
      ],
      "metadata": {
        "id": "_7oaq9eDSWes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define Logistic Regression with cross-validation for C\n",
        "logreg_cv = LogisticRegressionCV(\n",
        "    Cs=10,                # Try 10 different C values\n",
        "    cv=5,                 # 5-fold cross-validation\n",
        "    penalty='l2',         # L2 regularization\n",
        "    solver='liblinear',   # Works well for small datasets\n",
        "    scoring='accuracy',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "logreg_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = logreg_cv.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Results\n",
        "print(\"Optimal C value found:\", logreg_cv.C_[0])\n",
        "print(\"Test Accuracy: {:.4f}\".format(accuracy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMbIJl_CSjC6",
        "outputId": "e994079a-8d28-45df-93f2-cac08e2e5449"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal C value found: 0.3593813663804626\n",
            "Test Accuracy: 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q.25 Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.'''"
      ],
      "metadata": {
        "id": "3s4Nc77kSlfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Save model using joblib\n",
        "joblib.dump(model, 'logistic_model.pkl')\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "print(\"Model and scaler saved successfully!\")\n",
        "\n",
        "# Step 6: Load model and scaler\n",
        "loaded_model = joblib.load('logistic_model.pkl')\n",
        "loaded_scaler = joblib.load('scaler.pkl')\n",
        "\n",
        "# Step 7: Predict using loaded model\n",
        "X_new_scaled = loaded_scaler.transform(X_test)  # Apply same scaling\n",
        "y_pred = loaded_model.predict(X_new_scaled)\n",
        "\n",
        "# Step 8: Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Loaded model accuracy: {:.4f}\".format(accuracy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTdh77mbSrZ_",
        "outputId": "6d0b904a-ee36-4be4-f47a-0e2fa6bbce10"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and scaler saved successfully!\n",
            "Loaded model accuracy: 0.9737\n"
          ]
        }
      ]
    }
  ]
}